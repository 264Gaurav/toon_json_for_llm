/**
 * Live LLM testing with Ollama
 * Sends both JSON and TOON formatted data to the model and compares responses
 * 
 * OLLAMA MODELS EXPLAINED:
 * - Ollama is a tool for running large language models locally
 * - It allows you to run LLMs on your own machine without API costs
 * - Popular models include:
 *   - llama3.1 / llama3.1:8b - Meta's Llama 3.1 (8B parameter version)
 *   - llama3 - Meta's Llama 3
 *   - mistral - Mistral AI's model
 *   - codellama - Code-specialized Llama model
 * - Models must be downloaded first: ollama pull llama3.1
 * - Ollama runs a local server that this script connects to
 * - This allows testing TOON format with real LLM responses
 */

// Import TOON encoder function
import { encode } from './toon.js';

// Import Ollama client library
// This is a Node.js package that provides API to interact with Ollama
import ollama from 'ollama';

// Sample data for testing
const testData = {
  products: [
    { id: 1, name: 'Wireless Mouse', price: 29.99, category: 'Electronics', inStock: true },
    { id: 2, name: 'Mechanical Keyboard', price: 89.99, category: 'Electronics', inStock: true },
    { id: 3, name: 'USB-C Hub', price: 45.00, category: 'Electronics', inStock: false },
    { id: 4, name: 'Monitor Stand', price: 39.99, category: 'Office', inStock: true },
    { id: 5, name: 'Desk Lamp', price: 24.99, category: 'Office', inStock: true },
    { id: 6, name: 'Ergonomic Chair', price: 249.99, category: 'Furniture', inStock: true },
    { id: 7, name: 'Standing Desk', price: 499.99, category: 'Furniture', inStock: false },
    { id: 8, name: 'Cable Management', price: 12.99, category: 'Office', inStock: true }
  ]
};

const jsonFormat = JSON.stringify(testData, null, 2);
const toonFormat = encode(testData, { delimiter: '\t' });


/**
 * Tests a specific Ollama model with both JSON and TOON formats
 * 
 * @param {string} modelName - Name of the Ollama model to test (e.g., 'llama3.1')
 * 
 * 'async' keyword: marks function as asynchronous
 * Async functions can use 'await' to wait for Promises to resolve
 * This is needed because LLM API calls are asynchronous (take time)
 */
async function testWithModel(modelName) {
  console.log(`\nü§ñ Testing with model: ${modelName}`);
  console.log('='.repeat(80));

  // Prompt template: instruction for the LLM
  // Template literal: backticks allow multi-line strings
  const prompt = `You are a helpful assistant. Analyze the product data below and list all products that are:
1. In the "Electronics" category
2. Currently in stock
3. Price is less than $50

Format your response as a simple list.`;

  // Test 1: JSON Format
  console.log('\nüìä Test 1: JSON Format');
  console.log('‚îÄ'.repeat(80));

  console.log("json data : ", jsonFormat)

  console.log('‚îÄ'.repeat(30));
  
  // Build prompt with JSON data
  // Template literal embeds jsonFormat variable
  // Markdown code blocks (```json) help LLM understand the format
  const jsonPrompt = `${prompt}\n\nProduct data in JSON format:\n\`\`\`json\n${jsonFormat}\n\`\`\``;
  
  // try-catch: handles errors from API calls
  try {
    // Date.now() - returns current timestamp in milliseconds
    // Used to measure response time
    const jsonStart = Date.now();
    
    // 'await' keyword: waits for Promise to resolve
    // ollama.chat() - sends chat request to Ollama API
    // Returns a Promise that resolves to response object
    const jsonResponse = await ollama.chat({
      model: modelName, // Which model to use
      messages: [
        // System message: sets the behavior/role of the assistant
        { role: 'system', content: 'You are a helpful assistant that analyzes data accurately.' },
        // User message: the actual prompt with data
        { role: 'user', content: jsonPrompt }
      ]
    });
    
    // Calculate elapsed time: current time - start time
    const jsonTime = Date.now() - jsonStart;
    
    // Log metrics from JSON test
    // Template literals: ${} embeds variables
    console.log(`‚è±Ô∏è  Response time: ${jsonTime}ms`);
    console.log(`üìù Response length: ${jsonResponse.message.content.length} chars`);
    
    // eval_count - number of tokens generated by model (may not always be available)
    // || operator: if eval_count is undefined/null, use 'N/A'
    console.log(`üî¢ Tokens used: ${jsonResponse.eval_count || 'N/A'}`);
    console.log('\nResponse:');
    
    // substring(0, 300) - gets first 300 characters
    // Ternary operator: if length > 300, append '...', else append ''
    console.log(jsonResponse.message.content.substring(0, 300) + (jsonResponse.message.content.length > 300 ? '...' : ''));
    
    // Store metrics in object for comparison
    // Object literal: { } creates object with properties
    const jsonMetrics = {
      time: jsonTime,                              // Response time in milliseconds
      contentLength: jsonResponse.message.content.length, // Length of response
      promptLength: jsonPrompt.length              // Length of input prompt
    };

    // Test 2: TOON Format
    console.log('\nüìä Test 2: TOON Format');
    console.log('‚îÄ'.repeat(80));

    console.log("toon data : ", toonFormat)

    console.log('‚îÄ'.repeat(30));
    
    const toonPrompt = `${prompt}\n\nProduct data in TOON format:\n\`\`\`toon\n${toonFormat}\n\`\`\``;
    
    const toonStart = Date.now();
    const toonResponse = await ollama.chat({
      model: modelName,
      messages: [
        { role: 'system', content: 'You are a helpful assistant that analyzes data accurately.' },
        { role: 'user', content: toonPrompt }
      ]
    });
    const toonTime = Date.now() - toonStart;
    
    console.log(`‚è±Ô∏è  Response time: ${toonTime}ms`);
    console.log(`üìù Response length: ${toonResponse.message.content.length} chars`);
    console.log(`üî¢ Tokens used: ${toonResponse.eval_count || 'N/A'}`);
    console.log('\nResponse:');
    console.log(toonResponse.message.content.substring(0, 300) + (toonResponse.message.content.length > 300 ? '...' : ''));
    
    const toonMetrics = {
      time: toonTime,
      contentLength: toonResponse.message.content.length,
      promptLength: toonPrompt.length
    };

    // Comparison: Calculate differences between JSON and TOON
    console.log('\nüìä Comparison');
    console.log('‚îÄ'.repeat(80));
    
    // Calculate percentage reduction in prompt size
    // Formula: ((old - new) / old) * 100
    // toFixed(1) - rounds to 1 decimal place
    const promptReduction = ((jsonMetrics.promptLength - toonMetrics.promptLength) / jsonMetrics.promptLength * 100).toFixed(1);
    
    // Calculate percentage difference in response time
    const timeDifference = ((toonMetrics.time - jsonMetrics.time) / jsonMetrics.time * 100).toFixed(1);
    
    // Display comparison results
    // Template literal with arrow (‚Üí) shows before ‚Üí after
    console.log(`Input size reduction: ${promptReduction}% (${jsonMetrics.promptLength} ‚Üí ${toonMetrics.promptLength} chars)`);
    console.log(`Response time difference: ${timeDifference}% (${jsonMetrics.time}ms ‚Üí ${toonMetrics.time}ms)`);
    
    // if-else: conditional execution
    // < operator: less than comparison
    if (toonMetrics.time < jsonMetrics.time) {
      console.log('‚úÖ TOON is faster!');
    } else {
      console.log('‚ö†Ô∏è  JSON is faster (may vary by test)');
    }

    return { jsonMetrics, toonMetrics };

  } catch (error) {
    console.error('‚ùå Error:', error.message);
    console.log('\nüí° Make sure Ollama is running and the model is available:');
    console.log('   ollama pull llama3.1');
    console.log('   ollama serve');
    return null;
  }
}

/**
 * Main function: orchestrates the LLM testing
 * 
 * This function:
 * 1. Checks if Ollama is running
 * 2. Lists available models
 * 3. Tries to find a preferred model
 * 4. Runs comparison tests
 */
async function main() {
  console.log('üöÄ LLM Format Comparison Test');
  console.log('Testing JSON vs TOON with Ollama\n');
  
  // Check if Ollama is available
  // try-catch: handles connection errors
  try {
    // ollama.list() - returns list of available models
    // 'await' - waits for API call to complete
    const models = await ollama.list();
    console.log('‚úÖ Ollama is running');
    
    // map() - transforms array: extracts name from each model object
    // join(', ') - joins array elements with comma and space
    console.log('Available models:', models.models.map(m => m.name).join(', '));
    
    // Try different models in order of preference
    // Array of model names to try (in priority order)
    // 'llama3.1' - latest Llama model (most preferred)
    // 'llama3.1:8b' - 8 billion parameter version
    // 'llama3' - previous Llama version
    // 'mistral' - Mistral AI model
    // 'codellama' - Code-specialized model
    const modelsToTry = [
      'llama3.1',
      'llama3.1:8b',
      'llama3',
      'mistral',
      'codellama'
    ];
    
    // 'let' keyword: declares variable that can be reassigned
    // (unlike 'const' which cannot be reassigned)
    let modelFound = false;
    
    // for...of loop: iterate through preferred models
    for (const model of modelsToTry) {
      // some() - returns true if any element passes test
      // includes() - checks if string contains substring
      // split(':')[0] - splits by ':' and gets first part (e.g., 'llama3.1:8b' ‚Üí 'llama3.1')
      const available = models.models.some(m => m.name.includes(model.split(':')[0]));
      
      // if statement: execute if model is available
      if (available) {
        // find() - returns first element matching condition
        // ?. optional chaining: if find() returns undefined, don't access .name
        // || operator: if left side is falsy, use right side
        const actualModel = models.models.find(m => m.name.includes(model.split(':')[0]))?.name || model;
        
        // 'await' - wait for test to complete
        await testWithModel(actualModel);
        
        // Set flag to indicate model was found
        modelFound = true;
        
        // 'break' keyword: exits the loop immediately
        break;
      }
    }
    
    // If no preferred model found, use fallback
    // ! operator: logical NOT (negation)
    if (!modelFound) {
      console.log('\n‚ö†Ô∏è  None of the preferred models found. Using first available model.');
      
      // Check if any models are available
      // Array.length - number of elements
      if (models.models.length > 0) {
        // Use first available model
        // [0] - access first element of array
        await testWithModel(models.models[0].name);
      } else {
        // No models available - show instructions
        console.log('‚ùå No models available. Please install a model:');
        console.log('   ollama pull llama3.1');
      }
    }
    
  } catch (error) {
    // catch block: handles errors from try block
    // console.error() - logs error message
    console.error('‚ùå Ollama is not running or not available');
    console.log('\nüí° To get started:');
    console.log('   1. Install Ollama: https://ollama.ai');
    console.log('   2. Download a model: ollama pull llama3.1');
    console.log('   3. Start Ollama: ollama serve');
    console.log('   4. Run this script again');
  }
  
  console.log('\n‚úÖ Test complete!');
}

// Call main function and catch any unhandled errors
// .catch() - Promise method: handles errors that occur in async function
// console.error - logs error to console
main().catch(console.error);
